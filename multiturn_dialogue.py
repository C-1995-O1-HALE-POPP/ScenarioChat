# ä¸€ä¸ªé¢å¤–çš„å¤šè½®å¯¹è¯å®ç°ï¼Œå­—é¢æ„ä¹‰çš„ä½¿ç”¨ä¸¤ä¸ªllmäº’ç›¸è¯¢é—®ã€‚
# æŒ‰ç…§å®˜ç½‘apiæ•™ç¨‹ä½¿ç”¨assistantæºå¸¦ä¸Šä¸‹æ–‡å®ç°è®°å¿†åŠ›æœºåˆ¶ã€‚

import os
import sys
import argparse
from http import HTTPStatus
from typing import List, Dict, Any

import dashscope
from dashscope import Generation
from dashscope.api_entities.dashscope_response import Role  # è§’è‰²å¸¸é‡
from loguru import logger

# --------------------------------------------------------------------------------------
# é€šç”¨ LLM è°ƒç”¨å°è£…
# --------------------------------------------------------------------------------------
def _extract_content(resp: Any) -> str:
    """
    ä» DashScope çš„å“åº”å¯¹è±¡ä¸­æŠ½å– assistant å†…å®¹ã€‚
    åŒæ—¶å…¼å®¹ prompt / messages ä¸¤ç§è°ƒç”¨å½¢æ€ã€‚
    """
    if resp.status_code != HTTPStatus.OK:
        raise RuntimeError(f"DashScope error {resp.status_code}: {resp.message}")

    out = resp.output

    # â‘  prompt æ–¹å¼è¿”å›çº¯å­—ç¬¦ä¸²
    if isinstance(out, str):
        return out.strip()

    # â‘¡ messages æ–¹å¼ result_format='message'ï¼Œè¿”å› dict
    if isinstance(out, dict):
        try:
            return out["choices"][0]["message"]["content"].strip()
        except (KeyError, IndexError):
            pass

    # â‘¢ å…¶å®ƒæ„å¤–æ ¼å¼ â€”â€” é€€åŒ–ä¸º str(out)
    return str(out).strip()


def call_llm(
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 0.7,
) -> str:
    """
    å•è½®è°ƒç”¨ DashScope Generation APIï¼ˆåŒæ­¥ï¼Œéæµå¼ï¼‰ã€‚
    :param model:  Generation.Models.<model_name>  æˆ–ç›´æ¥å¡«å­—ç¬¦ä¸²æ¨¡å‹å
    :param messages: OpenAI Chat æ ¼å¼çš„å†å²æ¶ˆæ¯
    :return: assistant å›å¤æ–‡æœ¬
    """
    # Convert list of dicts to list of Message objects
    from dashscope.api_entities.dashscope_response import Message
    message_objs = [Message(role=m["role"], content=m["content"]) for m in messages]
    logger.debug(f"Calling model: {model}, with messages: {message_objs}")
    resp = Generation.call(
        model=model,
        messages=message_objs,
        result_format="message",   # è¦æ±‚è¿”å› OpenAI ChatMessage ç»“æ„
        temperature=temperature,
        stream=False,              # éœ€è¦æµå¼å¯æ”¹ True
    )
    return _extract_content(resp)


# --------------------------------------------------------------------------------------
# å¤šè½®å¯¹è¯æ ¸å¿ƒ
# --------------------------------------------------------------------------------------
def run_multi_turn_dialog(
    turns: int,
    background: str,
    init_user_prompt: str | None = None,
    user_model: str = Generation.Models.qwen_turbo,
    assistant_model: str = Generation.Models.qwen_plus,
    temperature: float = 0.7,
) -> List[Dict[str, str]]:
    """
    è®© user_model å’Œ assistant_model è¿›è¡Œå¤šè½®å¯¹è¯ã€‚
    ä¸€è½® = (user â†’ assistant)ã€‚
    :return: å®Œæ•´èŠå¤©è®°å½•ï¼ˆlist[dict]ï¼‰
    """
    # å…¨å±€ System èƒŒæ™¯
    system_msg = {"role": Role.SYSTEM, "content": background}
    history: List[Dict[str, str]] = [system_msg]

    # -------- â‘  é¦–å¥ç”Ÿæˆæˆ–æ³¨å…¥ --------
    if init_user_prompt:
        history.append({"role": Role.USER, "content": init_user_prompt})
    else:
        # å¼•å¯¼ Qwen-Turbo è¯´å‡ºç¬¬ä¸€å¥â€œå­¦ç”Ÿå‘é—®â€
        primer = {
            "role": Role.SYSTEM,
            "content": "ä½ æ­£åœ¨æ‰®æ¼”ä¸€åæœ¬ç§‘ç”Ÿï¼Œè¯·è¯´å‡ºç¬¬ä¸€å¥å¯¹è¯ã€‚åªè¿”å›ç”¨æˆ·çš„è¯ã€‚",
        }
        user_first = call_llm(
            user_model,
            messages=[system_msg, primer],
            temperature=temperature,
        )
        history.append({"role": Role.USER, "content": user_first})

    # -------- â‘¡ ä¸»å¾ªç¯ --------
    for _ in range(turns):
        # åŠ©ç†å›å¤
        assistant_reply = call_llm(
            assistant_model,
            messages=history,
            temperature=temperature,
        )
        history.append({"role": Role.ASSISTANT, "content": assistant_reply})

        # æ¨¡æ‹Ÿç”¨æˆ·è¿½é—®
        user_followup = call_llm(
            user_model,
            messages=history + [
                {
                    "role": Role.SYSTEM,
                    "content": "è¯·ç»§ç»­æ‰®æ¼”ç”¨æˆ·ï¼Œç”¨å£è¯­æ–¹å¼å›åº”ä¸Šé¢åŠ©ç†çš„å›ç­”ï¼Œåªè¿”å›ä¸‹ä¸€å¥ã€‚",
                }
            ],
            temperature=temperature,
        )
        history.append({"role": Role.USER, "content": user_followup})

    return history


# --------------------------------------------------------------------------------------
# CLI / Demo
# --------------------------------------------------------------------------------------
def _cli() -> None:
    parser = argparse.ArgumentParser(description="Qwen Multi-Agent Chat (DashScope)")
    parser.add_argument("--turns", type=int, default=3, help="å¯¹è¯è½®æ•°ï¼ˆuser+assistant ä¸º 1 è½®ï¼‰")
    parser.add_argument(
        "--background",
        type=str,
        default=(
            "åœºæ™¯ï¼šè®¨è®ºå¦‚ä½•åœ¨ FPGA é¡¹ç›®ä¸­å®ç° DDR3 å¸§ç¼“å†²è¯»å†™ä¸åŒç¼“å†²ã€‚\n"
            "æ¨¡æ‹Ÿç”¨æˆ·ï¼šå¤§ä¸‰å­¦ç”Ÿï¼Œç•¥æ‡‚ä½†ä»æœ‰ç–‘æƒ‘ï¼›"
            "åŠ©ç†ï¼šèµ„æ·±ç¡¬ä»¶å·¥ç¨‹å¸ˆï¼Œéœ€è¦å¾ªåºæ¸è¿›åœ°è§£ç­”å¹¶ç©¿æ’ä»£ç ç¤ºä¾‹ä¸åŸç†è¯´æ˜ã€‚"
        ),
        help="å¯¹è¯èƒŒæ™¯è®¾å®šï¼ˆSystem Promptï¼‰",
    )
    parser.add_argument("--user_model", type=str, default="qwen-turbo", help="ç”¨æˆ·æ¨¡å‹å")
    parser.add_argument("--assistant_model", type=str, default="qwen-plus", help="åŠ©ç†æ¨¡å‹å")
    args = parser.parse_args()

    # å…è®¸å­—ç¬¦ä¸²æˆ– Generation.Models æšä¸¾
    user_model = (
        getattr(Generation.Models, args.user_model)
        if hasattr(Generation.Models, args.user_model)
        else args.user_model
    )
    assistant_model = (
        getattr(Generation.Models, args.assistant_model)
        if hasattr(Generation.Models, args.assistant_model)
        else args.assistant_model
    )
    logger.info(f"Using User Model: {user_model}, Assistant Model: {assistant_model}")

    dialog = run_multi_turn_dialog(
        turns=args.turns,
        background=args.background,
        init_user_prompt=None,
        user_model=user_model,
        assistant_model=assistant_model,
    )

    for msg in dialog:
        role = "ğŸ‘¤User" if msg["role"] == Role.USER else "ğŸ¤–Assistant"
        print(f"\n[{role}]: {msg['content']}")


if __name__ == "__main__":
    # è‹¥æœªé…ç½® API-KEYï¼Œè„šæœ¬ç›´æ¥é€€å‡º
    if not os.getenv("DASHSCOPE_API_KEY"):
        sys.exit("âŒ  è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYï¼Œå†è¿è¡Œæ­¤è„šæœ¬ï¼")
    _cli()
